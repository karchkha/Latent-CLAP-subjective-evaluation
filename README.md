# Latent CLAP Loss for Better Foley Sound Synthesis

Audio samples for the paper "_[Latent CLAP Loss for Better Foley Sound Synthesis](arxiv link here)_".

## Abstract

Foley sound generation, the art of creating audio for multimedia, has recently seen notable advancements through text-conditioned latent diffusion models. These systems use multimodal text-audio representation models, such as Contrastive Language-Audio Pretraining (CLAP), whose objective is to map corresponding audio and text prompts into a joint embedding space. AudioLDM, a text-to-audio model, was the winner of the DCASE2023 task 7 Foley sound synthesis challenge. The winning system fine-tuned the model for specific audio classes and applied a post-filtering method using CLAP similarity scores between output audio and input text at inference time, requiring the generation of extra samples, thus reducing data generation efficiency. We introduce a new loss term to enhance Foley sound generation in AudioLDM without post-filtering. This loss term uses a new module based on the CLAP model—Latent CLAP encoder—to align the latent diffusion output with real audio in a shared CLAP embedding space. Our experiments demonstrate that our method effectively reduces the Fréchet Audio Distance (FAD) score of the generated audio and eliminates the need for post-filtering, thus enhancing generation efficiency.

## Organization

Audio files are organized using the following folder structure:
```
audio
└── model_name
    └── Class_name
           ├── 01.wav
           ├── 02.wav
           └── 03.wav
 ```
#### `model_name`
Corresponds to the model name from the paper.
+ `baseline` -> baseline (LDM + Tuning)
+ `GT` -> Ground Truth audios (Ground Truth)
+ `new` -> Proposed model (LDM + Tuning + Latent)


Each folder contains three 16kHz `.wav` files.

## Subjective Evaluation

Human perception of audio quality and class relevance can be highly subjective and nonlinear, often correlating poorly with objective metrics~\cite{vinay2022evaluating}. Human ratings are still considered the gold standard in audio quality evaluation. Thus to better ground our work, the proposed method was compared to audio generated by the baseline model involving the tuning layer (LDM + Tuning)~\cite{yuan2023latent} and ground truth samples in a series of 7 online surveys. Each survey focused on a specific sound class, included 15 audio samples from each of the three models and was completed by 40 listeners. Respondents listened to one audio clip at a time and rated their quality and category fit on a scale from 0 to 10, where 0 indicates the lowest possible quality or fit to category, and 10 indicates the highest. Audio clips were presented in randomized order.

As reported in Table~\ref{tab_subjective_evaluation}, the average subjective ratings reveal that the Latent CLAP loss model outscored the baseline model and ground truth samples in both audio quality and category fitness. A mixed-design ANOVA showed that the main effect of model was significant for both audio quality ($F$(1.9,509.6) = 115.3, $p$ $<$ .001, $\eta_{p}^{2}$ = .3) and category fit ($F$(1.9,522) = 156.6, $p$ $<$ .001, $\eta_{p}^{2}$ = .36), while post-hoc paired t-tests with Bonferroni corrections further support the finding that the proposed model was rated significantly higher than both the baseline model and ground truth samples across both items ($p$ $<$ .001 for all comparisons). 

The lower ratings for the ground truth samples is unexpected and could be attributed to the fact that real-world recordings often contain extraneous noises or recording artifacts. Our generated audio presents cleaner sounds with features that are more distinctly aligned with the target class, offering a potentially clearer representation of the intended sound event, which may contribute to higher scores in human evaluations.
